{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence for Cybersecurity Project\n",
    "### Dataset used: [Malicious URLs dataset by Manu Siddhartha](https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset)\n",
    "\n",
    "### Candidates: Riccardo Fantasia & Leonardo Pantani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_raw = pd.read_csv(\"UNIPI-IA-dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = [\"malware\", \"phishing\", \"defacement\", \"benign\"]\n",
    "\n",
    "df_raw[\"type\"] = pd.Categorical(df_raw[\"type\"], categories=types, ordered=True)\n",
    "df = df_raw.sort_values(by=\"type\").drop_duplicates(subset=\"url\", keep=\"first\")\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import graph_objects as go\n",
    "\n",
    "total_count = df['type'].shape[0]\n",
    "count = df['type'].value_counts()\n",
    "percentages = (count / total_count * 100).round(2)\n",
    "colors = ['#FF6633', '#FFB399', '#FF33FF', '#FFFF99', '#00B3E6', '#E6B333', '#3366E6', '#999966', '#99FF99', '#B34D4D']\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(x=count.index, y=count, marker=dict(color=colors), text=[f\"{p}%\" for p in percentages], textposition='outside', textfont=dict(color='white'))])\n",
    "fig.update_layout(xaxis_title='Types', yaxis_title='Count', title='Count of Different Types of URLs', plot_bgcolor='black', paper_bgcolor='black', font=dict(color='white'))\n",
    "fig.update_xaxes(tickfont=dict(color='white'))\n",
    "fig.update_yaxes(tickfont=dict(color='white'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estriamo le 23 features da ogni url nel dataset intero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.features_extractors import (extract_feature_ip_use,extract_feature_url_entropy,extract_feature_num_digits,extract_feature_url_length,extract_feature_num_query_parameters,extract_feature_num_fragments,extract_feature_num_percent20,extract_feature_num_at_signs,extract_feature_hashttp,extract_feature_hashttps,extract_feature_dot_number,extract_feature_num_www, extract_feature_directory_num,extract_feature_embed_domain_number,extract_feature_suspiciousurl,extract_feature_count_percent,extract_feature_count_dash,extract_feature_count_equal,extract_feature_is_shortened,extract_feature_hostname_length,extract_feature_first_directory_length,extract_feature_top_level_domain_length,extract_feature_letter_count)\n",
    "\n",
    "df['feature_ip_use'] = df['url'].apply(extract_feature_ip_use)\n",
    "df['feature_url_entropy'] = df['url'].apply(extract_feature_url_entropy)\n",
    "df['feature_num_digits'] = df['url'].apply(extract_feature_num_digits)\n",
    "df['feature_url_length'] = df['url'].apply(extract_feature_url_length)\n",
    "df['feature_num_query_parameters'] = df['url'].apply(extract_feature_num_query_parameters)\n",
    "df['feature_num_fragments'] = df['url'].apply(extract_feature_num_fragments)\n",
    "df['feature_num_percent20'] = df['url'].apply(extract_feature_num_percent20)\n",
    "df['feature_num_at_signs'] = df['url'].apply(extract_feature_num_at_signs)\n",
    "df['feature_hashttp'] = df['url'].apply(extract_feature_hashttp)\n",
    "df['feature_hashttps'] = df['url'].apply(extract_feature_hashttps)\n",
    "df['feature_dot_number'] = df['url'].apply(extract_feature_dot_number)\n",
    "df['feature_num_www'] = df['url'].apply(extract_feature_num_www)\n",
    "df['feature_directory_num'] = df['url'].apply(extract_feature_directory_num)\n",
    "df['feature_embed_domain_number'] = df['url'].apply(extract_feature_embed_domain_number)\n",
    "df['feature_suspiciousurl'] = df['url'].apply(extract_feature_suspiciousurl)\n",
    "df['feature_count_percent'] = df['url'].apply(extract_feature_count_percent)\n",
    "df['feature_count_dash'] = df['url'].apply(extract_feature_count_dash)\n",
    "df['feature_count_equal'] = df['url'].apply(extract_feature_count_equal)\n",
    "df['feature_is_shortened'] = df['url'].apply(extract_feature_is_shortened)\n",
    "df['feature_hostname_length'] = df['url'].apply(extract_feature_hostname_length)\n",
    "df['feature_first_directory_length'] = df['url'].apply(extract_feature_first_directory_length)\n",
    "df['feature_top_level_domain_length'] = df['url'].apply(extract_feature_top_level_domain_length)\n",
    "df['feature_letter_count'] = df['url'].apply(extract_feature_letter_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostriamo una HeatMap di correlazione tra le feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filtra solo le colonne con prefisso \"feature\"\n",
    "features_df = df[[col for col in df.columns if col.startswith('feature')]]\n",
    "features_df = features_df.rename(columns=lambda x: x.replace('feature_', ''))\n",
    "\n",
    "# Crea e mostra la heatmap\n",
    "correlation_matrix = features_df.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, square=True)\n",
    "plt.title(\"Heatmap delle Correlazioni tra le Feature\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data la forte correlazione tra:\n",
    "- embed_domain_number <-> hashttp\n",
    "- count_equal <-> num_query_parameters\n",
    "- url_length <-> letter_count\n",
    "\n",
    "... e data la stretta somiglianza logica delle funzioni abbiamo deciso di rimuovere:\n",
    "- embed_domain_number\n",
    "- count_equal\n",
    "- url_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot di count_equal <-> num_query_parameters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(features_df['count_equal'], features_df['num_query_parameters'], alpha=0.7, color='orange')\n",
    "plt.title('Scatter Plot: count_equals vs num_query_parameters')\n",
    "plt.xlabel('count_equals')\n",
    "plt.ylabel('num_query_parameters')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot di url_length <-> letter_count\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(features_df['url_length'], features_df['letter_count'], alpha=0.7, color='green')\n",
    "plt.title('Scatter Plot: url_length vs letter_count')\n",
    "plt.xlabel('url_length')\n",
    "plt.ylabel('letter_count')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come suggerito dal professore, facciamo un box plot per ogni classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lista delle feature\n",
    "features = [\n",
    "    'feature_url_entropy', 'feature_num_digits', 'feature_url_length', \n",
    "    'feature_num_query_parameters', 'feature_num_fragments', 'feature_num_percent20', \n",
    "    'feature_num_at_signs', 'feature_dot_number', 'feature_num_www', 'feature_directory_num', \n",
    "    'feature_embed_domain_number', 'feature_count_percent', 'feature_count_dash', \n",
    "    'feature_count_equal', 'feature_hostname_length', 'feature_first_directory_length', \n",
    "    'feature_top_level_domain_length', 'feature_letter_count'\n",
    "]\n",
    "\n",
    "# Classi nel dataset\n",
    "classes = df[\"type\"].unique()\n",
    "\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Creazione del box plot per ogni classe\n",
    "    data = [df[df[\"type\"] == class_label][feature].dropna() for class_label in classes]\n",
    "    plt.boxplot(data, vert=False, patch_artist=True, showmeans=True, labels=classes)\n",
    "    \n",
    "    # Configurazione del grafico\n",
    "    plt.title(f'Box Plot of {feature} by Class', fontsize=12)\n",
    "    plt.xlabel(feature, fontsize=10)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facciamo un box plot per ogni feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['feature_url_entropy', 'feature_num_digits', 'feature_url_length', 'feature_num_query_parameters', 'feature_num_fragments', 'feature_num_percent20', 'feature_num_at_signs', 'feature_dot_number', 'feature_num_www', 'feature_directory_num', 'feature_embed_domain_number', 'feature_count_percent', 'feature_count_dash', 'feature_count_equal', 'feature_hostname_length', 'feature_first_directory_length', 'feature_top_level_domain_length', 'feature_letter_count']\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(12, 1.5))\n",
    "    plt.boxplot(df[feature].dropna(), vert=False, patch_artist=True, showmeans=True)\n",
    "    plt.title(f'Box Plot of {feature}', fontsize=10)\n",
    "    plt.xlabel(feature, fontsize=8)\n",
    "    plt.gca().yaxis.set_visible(False)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimuoviamo le seguenti feature:\n",
    "- embed_domain_number\n",
    "- count_equal\n",
    "- url_length\n",
    "\n",
    "Testeremo successivamente allenando il modello di classificazione sul dataset originale e su quello ripulito dalle suddette feature, per valutare l'effettivo miglioramento in termini di classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"feature_embed_domain_number\", \"feature_count_equal\", \"feature_url_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filtra solo le colonne con prefisso \"feature\"\n",
    "features_df = df[[col for col in df.columns if col.startswith('feature')]]\n",
    "features_df = features_df.rename(columns=lambda x: x.replace('feature_', ''))\n",
    "\n",
    "# Crea e mostra la heatmap\n",
    "correlation_matrix = features_df.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, square=True)\n",
    "plt.title(\"Heatmap delle Correlazioni tra le Feature\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo ora per i nostri scopi successivi una funzione per ripulire il dataset da alcuni outlier. Decidiamo di impostare un treshold abbastanza conservativo (3xIQR) data la natura dei numerevoli outlier, che non derivano da errori di misura, ma rappresentano possibili URL malevoli o casi limite. Rimuoverli in modo troppo aggressivo (1.5xIQR) rischierebbe di perdere esempi rilevanti per il modello. La soglia più ampia massimizza la conservazione di dati potenzialmente informativi pur riducendo valori estremi eccessivi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    df_clean = df.copy()\n",
    "    columns=['feature_url_entropy', 'feature_num_digits', 'feature_dot_number', 'feature_directory_num', 'feature_hostname_length', 'feature_first_directory_length', 'feature_top_level_domain_length', 'feature_letter_count']\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 3 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        df_clean = df_clean[\n",
    "            (df_clean[column] >= lower_bound) & \n",
    "            (df_clean[column] <= upper_bound)\n",
    "        ]\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suddividiamo il dataset in training e test set con un rapporto 80% e 20%, sia per il caso senza outlier che con."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from utils.utils import printInfo\n",
    "\n",
    "train_ratio = 0.80\n",
    "test_ratio = 0.20\n",
    "x_train_unbalanced, x_test, y_train_unbalanced, y_test = train_test_split(df.drop(columns=[\"type\", \"url\"]).copy(), df[\"type\"].copy(), test_size=1-train_ratio, shuffle=True, stratify=df[\"type\"].copy())\n",
    "\n",
    "printInfo(\"training\", y_train_unbalanced)\n",
    "printInfo(\"test\", y_test)\n",
    "\n",
    "#rimozione degli outliers\n",
    "x_train_clean_unbalanced = remove_outliers(x_train_unbalanced)\n",
    "y_train_clean_unbalanced = y_train_unbalanced[x_train_clean_unbalanced.index]\n",
    "\n",
    "print(\"\\nShape of x_train_unbalanced before removing outliers:\", x_train_unbalanced.shape)\n",
    "print(\"\\nShape of x_train_clean_unbalanced after removing outliers:\", x_train_clean_unbalanced.shape)\n",
    "\n",
    "# Crea boxplot per ogni feature del dataset pulito\n",
    "features = x_train_clean_unbalanced.columns\n",
    "\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(12, 1.5))\n",
    "    plt.boxplot(x_train_clean_unbalanced[feature].dropna(), \n",
    "                vert=False, \n",
    "                patch_artist=True, \n",
    "                showmeans=True)\n",
    "    plt.title(f'Box Plot of {feature} (after outlier removal)', fontsize=10)\n",
    "    plt.xlabel(feature, fontsize=8)\n",
    "    plt.gca().yaxis.set_visible(False)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un aspetto che notiamo dall'output soprastante è lo sbilanciamento, in termini di numero di sample, della classe malevola rispetto alla benigna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def balance_data_undersample_benign(x_train, y_train):\n",
    "    class_counts = Counter(y_train)\n",
    "    other_classes_count = sum([class_counts[label] for label in class_counts if label != \"benign\"])\n",
    "    benign_target = other_classes_count\n",
    "    undersampling_strategy = {\"benign\": benign_target}\n",
    "    rus = RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=42)\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train)\n",
    "    return x_train_resampled, y_train_resampled\n",
    "\n",
    "x_train, y_train = balance_data_undersample_benign(x_train_unbalanced, y_train_unbalanced)\n",
    "x_train_clean, y_train_clean = balance_data_undersample_benign(x_train_clean_unbalanced, y_train_clean_unbalanced) #senza outliers\n",
    "printInfo(\"training bilanciato\", y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nell'ottica di una classificazione eseguita mantenendo gli outliers, si procede inizialmente con una normalizzazione RobustScaler che mantiene meglio l'effetto degli outliers. Dopodiché si procederà con una Z-Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler_with_outliers = RobustScaler()\n",
    "scaler_clean = RobustScaler()\n",
    "\n",
    "#dati con outliers\n",
    "x_train_scaled_robust = scaler_with_outliers.fit_transform(x_train)\n",
    "x_test_scaled_robust = scaler_with_outliers.transform(x_test)\n",
    "\n",
    "#dati senza outliers\n",
    "x_train_clean_scaled_robust = scaler_clean.fit_transform(x_train_clean)\n",
    "\n",
    "\n",
    "\n",
    "x_train_scaled_robust = pd.DataFrame(x_train_scaled_robust, columns=x_train.columns, index=x_train.index)\n",
    "\n",
    "x_test_scaled_robust = pd.DataFrame(x_test_scaled_robust,columns=x_test.columns,index=x_test.index)\n",
    "\n",
    "x_train_clean_scaled_robust = pd.DataFrame(x_train_clean_scaled_robust,columns=x_train_clean.columns,index=x_train_clean.index)\n",
    "\n",
    "\n",
    "print(\"Statistiche descrittive con outliers:\")\n",
    "print(x_train_scaled_robust.describe())\n",
    "print(\"\\n ----------------------------------------------------------\")\n",
    "\n",
    "print(\"Statistiche descrittive senza outliers:\")\n",
    "print(x_train_clean_scaled_robust.describe())\n",
    "\n",
    "# Visualizzazione con boxplot\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(data=x_train_scaled_robust)\n",
    "plt.title('Distribuzione delle features dopo RobustScaler (caso con outliers)', fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(data=x_train_clean_scaled_robust)\n",
    "plt.title('Distribuzione delle features dopo RobustScaler (caso senza outliers)', fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora Z-Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Definire due scaler diversi\n",
    "scaler_with_outliers = StandardScaler()\n",
    "scaler_clean = StandardScaler()\n",
    "\n",
    "# 2. Scalare i dati con outliers\n",
    "x_train_scaled_standard = scaler_with_outliers.fit_transform(x_train)\n",
    "\n",
    "\n",
    "# 3. Scalare i dati senza outliers\n",
    "x_train_clean_scaled_standard = scaler_clean.fit_transform(x_train_clean)\n",
    "\n",
    "x_test_scaled_standard = scaler_clean.transform(x_test)\n",
    "\n",
    "# 4. Convertire tutto in DataFrame mantenendo colonne e indici\n",
    "x_train_scaled_standard = pd.DataFrame(\n",
    "    x_train_scaled_standard, \n",
    "    columns=x_train.columns, \n",
    "    index=x_train.index\n",
    ")\n",
    "\n",
    "\n",
    "x_train_clean_scaled_standard = pd.DataFrame(\n",
    "    x_train_clean_scaled_standard,\n",
    "    columns=x_train_clean.columns,\n",
    "    index=x_train_clean.index\n",
    ")\n",
    "\n",
    "x_test_scaled_standard = pd.DataFrame(\n",
    "    x_test_scaled_standard,\n",
    "    columns=x_test.columns,\n",
    "    index=x_test.index\n",
    ")\n",
    "\n",
    "# Statistiche descrittive e visualizzazione\n",
    "print(\"Statistiche con outliers:\")\n",
    "print(x_train_scaled_standard.describe())\n",
    "print(\"\\nStatistiche senza outliers:\")\n",
    "print(x_train_clean_scaled_standard.describe())\n",
    "\n",
    "# Boxplot per confronto\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(data=x_train_clean_scaled_standard)\n",
    "plt.title('Distribuzione delle features dopo StandardScaler (senza outliers)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(data=x_train_scaled_standard)\n",
    "plt.title('Distribuzione delle features dopo StandardScaler (con outliers)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATORI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso con outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier_randomforest = RandomForestClassifier()\n",
    "classifier_randomforest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "k = 5  # Number of folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Store cross-validation results\n",
    "cv_accuracy = []\n",
    "cv_f1 = []\n",
    "cv_auc = []\n",
    "\n",
    "for train_index, val_index in stratified_kfold.split(x_train, y_train):\n",
    "    x_train_fold, x_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the current fold\n",
    "    classifier_randomforest.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "    # Predictions\n",
    "    y_val_pred = classifier_randomforest.predict(x_val_fold)\n",
    "    y_val_proba = classifier_randomforest.predict_proba(x_val_fold)\n",
    "\n",
    "    # Metrics\n",
    "    cv_accuracy.append(accuracy_score(y_val_fold, y_val_pred))\n",
    "    cv_f1.append(f1_score(y_val_fold, y_val_pred, average='weighted'))\n",
    "    cv_auc.append(roc_auc_score(y_val_fold, y_val_proba, multi_class='ovr'))\n",
    "\n",
    "# Average CV metrics\n",
    "print(\"Stratified K-Fold Cross Validation Results:\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_accuracy):.4f} \\u00b1 {np.std(cv_accuracy):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(cv_f1):.4f} \\u00b1 {np.std(cv_f1):.4f}\")\n",
    "print(f\"Mean AUC Score: {np.mean(cv_auc):.4f} \\u00b1 {np.std(cv_auc):.4f}\")\n",
    "\n",
    "# Original performance metrics\n",
    "y_pred = classifier_randomforest.predict(x_test)\n",
    "y_pred_proba = classifier_randomforest.predict_proba(x_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['benign', 'phishing', 'defacement', 'malware'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['benign', 'phishing', 'defacement', 'malware'],\n",
    "            yticklabels=['benign', 'phishing', 'defacement', 'malware'])\n",
    "plt.xlabel('Classe predetta')\n",
    "plt.ylabel('Classe reale')\n",
    "plt.title('Matrice di Confusione')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': x_train.columns,\n",
    "    'importance': classifier_randomforest.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance in Random Forest Classification')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso senza outliers, vediamo se ne ha beneficiato oppure no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier_randomforest_without_outliers = RandomForestClassifier()\n",
    "classifier_randomforest_without_outliers.fit(x_train_clean, y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "k = 5  # Number of folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Store cross-validation results\n",
    "cv_accuracy = []\n",
    "cv_f1 = []\n",
    "cv_auc = []\n",
    "\n",
    "for train_index, val_index in stratified_kfold.split(x_train, y_train):\n",
    "    x_train_fold, x_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the current fold\n",
    "    classifier_randomforest_without_outliers.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "    # Predictions\n",
    "    y_val_pred = classifier_randomforest_without_outliers.predict(x_val_fold)\n",
    "    y_val_proba = classifier_randomforest_without_outliers.predict_proba(x_val_fold)\n",
    "\n",
    "    # Metrics\n",
    "    cv_accuracy.append(accuracy_score(y_val_fold, y_val_pred))\n",
    "    cv_f1.append(f1_score(y_val_fold, y_val_pred, average='weighted'))\n",
    "    cv_auc.append(roc_auc_score(y_val_fold, y_val_proba, multi_class='ovr'))\n",
    "\n",
    "# Average CV metrics\n",
    "print(\"Stratified K-Fold Cross Validation Results:\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_accuracy):.4f} \\u00b1 {np.std(cv_accuracy):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(cv_f1):.4f} \\u00b1 {np.std(cv_f1):.4f}\")\n",
    "print(f\"Mean AUC Score: {np.mean(cv_auc):.4f} \\u00b1 {np.std(cv_auc):.4f}\")\n",
    "\n",
    "# Original performance metrics\n",
    "y_pred = classifier_randomforest_without_outliers.predict(x_test)\n",
    "y_pred_proba = classifier_randomforest_without_outliers.predict_proba(x_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['benign', 'phishing', 'defacement', 'malware'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['benign', 'phishing', 'defacement', 'malware'],\n",
    "            yticklabels=['benign', 'phishing', 'defacement', 'malware'])\n",
    "plt.xlabel('Classe predetta')\n",
    "plt.ylabel('Classe reale')\n",
    "plt.title('Matrice di Confusione')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo anche l'effetto dello strong scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier_randomforest_without_outliers = RandomForestClassifier()\n",
    "classifier_randomforest_without_outliers.fit(x_train_scaled_robust, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_pred = classifier_randomforest_without_outliers.predict(x_test)\n",
    "y_pred_proba = classifier_randomforest_without_outliers.predict_proba(x_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['benign', 'phishing', 'defacement', 'malware'])\n",
    "print(report)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'phishing', 'defacement', 'malware'], yticklabels=['benign', 'phishing', 'defacement', 'malware'])\n",
    "plt.xlabel('Classe predetta')\n",
    "plt.ylabel('Classe reale')\n",
    "plt.title('Matrice di Confusione')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le prestazioni inferiori del modello Random Forest applicato ai dati normalizzati con lo StrongScaler, rispetto a quelli trattati con la tua funzione di rimozione degli outliers, possono essere attribuite alla natura dei modelli basati su alberi. Il Random Forest non è sensibile alla scala delle feature, poiché le sue suddivisioni si basano direttamente sui valori grezzi delle variabili. L'uso di un metodo come lo StrongScaler, che riduce l'impatto degli outliers comprimendoli nell'intervallo interquartile, può attenuare segnali informativi cruciali presenti in questi valori estremi. D'altra parte, la funzione di rimozione degli outliers elimina completamente tali dati, portando a una perdita di informazioni rilevanti. Nel contesto del dataset in esame, in cui gli outliers rappresentano URL malevoli, mantenere tali punti dati è essenziale, poiché forniscono un contributo discriminativo significativo al modello. Questo spiega perché il Random Forest applicato ai dati grezzi, che preservano integralmente gli outliers, ottiene le migliori prestazioni, evidenziando l'importanza di trattare con attenzione i valori estremi nei dataset in cui non sono errori di misurazione, ma piuttosto dati rilevanti per il problema in analisi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION\n",
    "Data la natura di questo classificatore, è necessario categorizzare le classi phishing, defacement, malware in un'unica che considereremo come \"maligna\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa le classi in 'malignant' e 'benign'\n",
    "y_train = y_train.replace({\"malware\": \"malignant\", \"phishing\": \"malignant\", \"defacement\": \"malignant\", \"benign\": \"benign\"})\n",
    "y_train_clean = y_train_clean.replace({\"malware\": \"malignant\", \"phishing\": \"malignant\", \"defacement\": \"malignant\", \"benign\": \"benign\"})\n",
    "y_test = y_test.replace({\"malware\": \"malignant\", \"phishing\": \"malignant\", \"defacement\": \"malignant\", \"benign\": \"benign\"})\n",
    "\n",
    "printInfo(\"training\", y_train)\n",
    "printInfo(\"test\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con Robust Scaler (caso con outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier_logisticregression = LogisticRegression()\n",
    "classifier_logisticregression.fit(x_train_scaled_robust, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_pred = classifier_logisticregression.predict(x_test_scaled_robust)\n",
    "y_pred_proba = classifier_logisticregression.predict_proba(x_test_scaled_robust)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['benign', 'malignant'])\n",
    "print(report)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'malignant'], yticklabels=['benign', 'malignant'])\n",
    "plt.xlabel('Classe predetta')\n",
    "plt.ylabel('Classe reale')\n",
    "plt.title('Matrice di Confusione [con RobustScaler]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso senza outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier_logisticregression = LogisticRegression()\n",
    "classifier_logisticregression.fit(x_train_clean_scaled_robust, y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_pred = classifier_logisticregression.predict(x_test_scaled_robust)\n",
    "y_pred_proba = classifier_logisticregression.predict_proba(x_test_scaled_robust)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['benign', 'malignant'])\n",
    "print(report)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'malignant'], yticklabels=['benign', 'malignant'])\n",
    "plt.xlabel('Classe predetta')\n",
    "plt.ylabel('Classe reale')\n",
    "plt.title('Matrice di Confusione [con RobustScaler]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con Standard Scaler (Z-Normalization) caso con outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier_logisticregression = LogisticRegression()\n",
    "classifier_logisticregression.fit(x_train_scaled_standard, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_pred = classifier_logisticregression.predict(x_test_scaled_standard)\n",
    "y_pred_proba = classifier_logisticregression.predict_proba(x_test_scaled_standard)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['benign', 'malignant'])\n",
    "print(report)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'malignant'], yticklabels=['benign', 'malignant'])\n",
    "plt.xlabel('Classe predetta')\n",
    "plt.ylabel('Classe reale')\n",
    "plt.title('Matrice di Confusione [con Z-Normalization]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso senza outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier_logisticregression = LogisticRegression()\n",
    "classifier_logisticregression.fit(x_train_clean_scaled_standard, y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_pred = classifier_logisticregression.predict(x_test_scaled_standard)\n",
    "y_pred_proba = classifier_logisticregression.predict_proba(x_test_scaled_standard)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['benign', 'malignant'])\n",
    "print(report)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['benign', 'malignant'], yticklabels=['benign', 'malignant'])\n",
    "plt.xlabel('Classe predetta')\n",
    "plt.ylabel('Classe reale')\n",
    "plt.title('Matrice di Confusione [con Z-Normalization]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo osservare che il caso migliore è un random forest applicato ai dati con tutti gli outliers. Per quanto riguarda la logistic regression, il caso migliore osservato è quello con tutti gli outliers e un robust scaler, cioè lo scaler che penalizza di meno l'effetto degli outliers. Questi risultati suggeriscono quanto atteso: per quanto forte sia il contributo degli outliers e la loro deviazione significativa dalla media di alcune features, essi pur sempre sono dati che rappresentano esempi significativi per il modello, e non semplici errori di misurazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    max_samples='auto',\n",
    "    contamination='auto',  # o un valore specifico es. 0.1 per 10% outliers\n",
    "    random_state=42\n",
    ")\n",
    "iso_forest.fit(x_train)\n",
    "predictions = iso_forest.predict(x_test)\n",
    "\n",
    "outliers = x_test[predictions == -1]\n",
    "inliers = x_test[predictions == 1]\n",
    "\n",
    "print(f\"Numero totale di campioni: {len(predictions)}\")\n",
    "print(f\"Numero di outliers: {len(outliers)}\")\n",
    "print(f\"Numero di inliers: {len(inliers)}\")\n",
    "print(f\"Percentuale outliers: {(len(outliers)/len(predictions))*100:.2f}%\")\n",
    "\n",
    "# Compare outlier distribution by type\n",
    "outlier_types = y_test[predictions == -1].value_counts()\n",
    "inlier_types = y_test[predictions == 1].value_counts()\n",
    "\n",
    "print(\"Distribution of URL types in outliers:\")\n",
    "print(outlier_types)\n",
    "print(\"\\nPercentage of each type that are outliers:\")\n",
    "for type_name in y_test.unique():\n",
    "    type_total = len(y_test[y_test == type_name])\n",
    "    type_outliers = outlier_types.get(type_name, 0)\n",
    "    print(f\"{type_name}: {(type_outliers/type_total)*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analisi condotta tramite l'algoritmo Isolation Forest evidenzia che, sebbene gli outliers rappresentino una frazione minoritaria del dataset (9.07%), essi includono una percentuale sproporzionatamente elevata di URL malevoli, con particolare rilevanza per le categorie malware (32.84%) e defacement (12.37%). Questo risultato sottolinea come gli outliers non siano semplicemente anomalie casuali o rumore, ma piuttosto un sottoinsieme del dataset altamente rappresentativo di eventi malevoli. Ignorare tali osservazioni potrebbe essere la ragione che comporta una perdita di informazioni critiche per il modello. Infatti, aver includeso gli outliers ha migliorato le capacità del modello nel rilevare e classificare con precisione gli attacchi, in particolare quelle più rare o emergenti, confermando così la validità della tesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopkins statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random\n",
    "\n",
    "def hopkins_statistic(X, sample_size=None): # un bel po' lento (1 minuto circa)\n",
    "    if sample_size is None:\n",
    "        sample_size = int(0.1 * X.shape[0])\n",
    "\n",
    "    random_indices = random.sample(range(0, X.shape[0]), sample_size)\n",
    "    X_sample = X[random_indices, :]\n",
    "\n",
    "    X_random = np.random.uniform(np.min(X, axis=0), np.max(X, axis=0), (sample_size, X.shape[1]))\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(X)\n",
    "\n",
    "    distances_sample, _ = nbrs.kneighbors(X_sample, n_neighbors=2)\n",
    "    W = np.sum(distances_sample[:, 1])\n",
    "\n",
    "    distances_random, _ = nbrs.kneighbors(X_random, n_neighbors=1)\n",
    "    U = np.sum(distances_random[:, 0])\n",
    "\n",
    "    return W / (W + U)\n",
    "\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_numeric)\n",
    "\n",
    "hopkins_score = hopkins_statistic(df_scaled)\n",
    "print(f\"Hopkins Statistic: {hopkins_score:.4f}\") \n",
    "# Valore intorno a 1.0: tendenza a clustering\n",
    "# Valore intorno a 0.5: dati distribuiti casualmente\n",
    "# Valore intorno a 0.0: dati distribuiti uniformemente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "x_unsupervised = df.drop(columns=[\"url\", \"type\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_unsupervised_scaled = scaler.fit_transform(x_unsupervised)\n",
    "x_unsupervised_scaled_sample = resample(x_unsupervised_scaled, n_samples=10000)\n",
    "\n",
    "# Principal Component Analysis\n",
    "pca = PCA(n_components=2)\n",
    "x_unsupervised_scaled_sample_pca = pca.fit_transform(x_unsupervised_scaled_sample)\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init='auto')\n",
    "clusters = kmeans.fit_predict(x_unsupervised_scaled_sample_pca)\n",
    "\n",
    "def compute_silhouette_score(data, n_clusters):\n",
    "    model = KMeans(n_clusters=n_clusters, n_init='auto')\n",
    "    labels = model.fit_predict(data)\n",
    "    score = silhouette_score(data, labels)\n",
    "    return score\n",
    "\n",
    "k_values = range(2, 11)\n",
    "scores = [compute_silhouette_score(x_unsupervised_scaled_sample_pca, k) for k in k_values]\n",
    "\n",
    "for k, score in zip(k_values, scores):\n",
    "    print(f'Cluster K: {k} | Silhouette score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_unsupervised_scaled_sample_pca[:, 0], x_unsupervised_scaled_sample_pca[:, 1], c=clusters, cmap='viridis', s=10)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=100, marker='x')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('Risultati del clustering')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "# inertia in teoria misura quanto i punti all'interno di un cluster sono vicini al loro centroide\n",
    "\n",
    "k_values = range(1, 11)  # numero di cluster da testare\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, n_init='auto')  # inizializza KMeans\n",
    "    kmeans.fit(x_unsupervised)  # addestra il modello sui dati\n",
    "    inertia.append(kmeans.inertia_)  # aggiunge l'inertia alla lista\n",
    "\n",
    "plt.plot(k_values, inertia, marker='o')\n",
    "plt.xlabel('Numero di cluster (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Metodo del Gomito')\n",
    "plt.show()\n",
    "# fa sbrosciare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
