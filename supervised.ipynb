{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence for Cybersecurity Project\n",
    "### Dataset used: [Malicious URLs dataset by Manu Siddhartha](https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset)\n",
    "\n",
    "### Candidates: Riccardo Fantasia & Leonardo Pantani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing warnings from prints\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing raw dataset\n",
    "import pandas as pd\n",
    "df_raw = pd.read_csv(\"UNIPI-IA-dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the types in a specific order. This is because in case of duplicates, for example:\n",
    "# https://www.strange-site.com/youwonaprize- malware\n",
    "# https://www.strange-site.com/youwonaprize - benign\n",
    "#\n",
    "# we would remove all samples, except the first one. This was decided because, in this context,\n",
    "# it is preferable to have a False Positive (FP), manually \"correctable\" by the user, than a False Negative (FN),\n",
    "# which could expose the user to a malicious site.\n",
    "\n",
    "types = [\"malware\", \"phishing\", \"defacement\", \"benign\"]\n",
    "\n",
    "df_raw[\"type\"] = pd.Categorical(df_raw[\"type\"], categories=types, ordered=True)\n",
    "df = df_raw.sort_values(by=\"type\").drop_duplicates(subset=\"url\", keep=\"first\")\n",
    "\n",
    "print(f\"Samples in original dataset: {df_raw.shape[0]}\\nSamples in no-duplicates dataset: {df.shape[0]} ({((df_raw.shape[0] - df.shape[0]) / df_raw.shape[0]) * 100:.2f}% reduction)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show for each class the number of urls in the dataset\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "total_count = df['type'].shape[0]\n",
    "count = df['type'].value_counts()\n",
    "percentages = (count / total_count * 100).round(2)\n",
    "colors = ['#FF6633', '#FFB399', '#FF33FF', '#FFFF99', '#00B3E6', '#E6B333', '#3366E6', '#999966', '#99FF99', '#B34D4D']\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(x=count.index, y=count, marker=dict(color=colors), text=[f\"{p}%\" for p in percentages], textposition='outside', textfont=dict(color='white'))])\n",
    "fig.update_layout(xaxis_title='Labels', yaxis_title='no. of URLs', title='Counts for different labels', plot_bgcolor='black', paper_bgcolor='black', font=dict(color='white'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting 23 features\n",
    "from utils.features_extractors import (extract_feature_ip_use,extract_feature_url_entropy,extract_feature_num_digits,extract_feature_url_length,extract_feature_num_query_parameters,extract_feature_num_fragments,extract_feature_num_percent20,extract_feature_num_at_signs,extract_feature_has_http,extract_feature_has_https,extract_feature_dot_number,extract_feature_num_www, extract_feature_directory_num,extract_feature_embed_domain_number,extract_feature_suspiciousurl,extract_feature_count_percent,extract_feature_count_dash,extract_feature_count_equal,extract_feature_is_shortened,extract_feature_hostname_length,extract_feature_first_directory_length,extract_feature_top_level_domain_length,extract_feature_num_letters)\n",
    "\n",
    "df['ip_use'] = df['url'].apply(extract_feature_ip_use)\n",
    "df['url_entropy'] = df['url'].apply(extract_feature_url_entropy)\n",
    "df['num_digits'] = df['url'].apply(extract_feature_num_digits)\n",
    "df['url_length'] = df['url'].apply(extract_feature_url_length)\n",
    "df['num_query_parameters'] = df['url'].apply(extract_feature_num_query_parameters)\n",
    "df['num_fragments'] = df['url'].apply(extract_feature_num_fragments)\n",
    "df['num_percent20'] = df['url'].apply(extract_feature_num_percent20)\n",
    "df['num_at_signs'] = df['url'].apply(extract_feature_num_at_signs)\n",
    "df['has_http'] = df['url'].apply(extract_feature_has_http)\n",
    "df['has_https'] = df['url'].apply(extract_feature_has_https)\n",
    "df['dot_number'] = df['url'].apply(extract_feature_dot_number)\n",
    "df['num_www'] = df['url'].apply(extract_feature_num_www)\n",
    "df['directory_num'] = df['url'].apply(extract_feature_directory_num)\n",
    "df['embed_domain_number'] = df['url'].apply(extract_feature_embed_domain_number)\n",
    "df['suspiciousurl'] = df['url'].apply(extract_feature_suspiciousurl)\n",
    "df['count_percent'] = df['url'].apply(extract_feature_count_percent)\n",
    "df['count_dash'] = df['url'].apply(extract_feature_count_dash)\n",
    "df['count_equal'] = df['url'].apply(extract_feature_count_equal)\n",
    "df['is_shortened'] = df['url'].apply(extract_feature_is_shortened)\n",
    "df['hostname_length'] = df['url'].apply(extract_feature_hostname_length)\n",
    "df['first_directory_length'] = df['url'].apply(extract_feature_first_directory_length)\n",
    "df['top_level_domain_length'] = df['url'].apply(extract_feature_top_level_domain_length)\n",
    "df['num_letters'] = df['url'].apply(extract_feature_num_letters)\n",
    "\n",
    "print(f\"Total no. of features: { df.drop(columns=['type', 'url']).shape[1] }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing features heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "correlation_matrix = df.drop(columns=['type', 'url']).corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, square=True, mask = (np.abs(correlation_matrix) <= 0.9))\n",
    "plt.title(\"Heatmap of Correlations between Features\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will remove, given the heatmap, the features with a correlation >= 0.9. List:\n",
    "# embed_domain_number, count_equal, url_length, first_directory_length\n",
    "df = df.drop(columns=[\"embed_domain_number\", \"count_equal\", \"url_length\", \"first_directory_length\"])\n",
    "\n",
    "print(f\"Total no. of features after removal: { df.drop(columns=['type', 'url']).shape[1] }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we divide the dataset into training set (80%) and test set (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.utils import printInfo\n",
    "\n",
    "train_ratio = 0.80\n",
    "test_ratio = 0.20\n",
    "x_train_unbalanced, x_test, y_train_unbalanced, y_test = train_test_split(df.drop(columns=[\"type\", \"url\"]).copy(), df[\"type\"].copy(), test_size=1-train_ratio, shuffle=True, stratify=df[\"type\"].copy())\n",
    "\n",
    "printInfo(\"training with outliers\", y_train_unbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove outliers from the training set.\n",
    "# We decided to set a fairly conservative threshold (3x IQR) given the nature of the numerous outliers, which do not arise from measurement errors, \n",
    "# but represent possible malicious URLs or edge cases. Removing them too aggressively (1.5xIQR) would risk losing relevant examples for the model. \n",
    "# The larger threshold maximizes the retention of potentially informative data while reducing excessive extreme values.\n",
    "from utils.utils import remove_outliers\n",
    "\n",
    "x_train_clean_unbalanced = remove_outliers(x_train_unbalanced)\n",
    "y_train_clean_unbalanced = y_train_unbalanced[x_train_clean_unbalanced.index]\n",
    "\n",
    "printInfo(\"training without outliers\", y_train_clean_unbalanced)\n",
    "print(f\"Reduction of the training dataset by {x_train_unbalanced.shape[0] - x_train_clean_unbalanced.shape[0]} elements ({((x_train_unbalanced.shape[0] - x_train_clean_unbalanced.shape[0]) / x_train_unbalanced.shape[0]) * 100:.2f}% reduction)\")\n",
    "\n",
    "printInfo(\"test\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create two box plots for each of these features. The first is for the dataset with outliers, the second without them.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = ['url_entropy', 'num_digits', 'num_query_parameters', 'num_fragments', 'num_percent20', 'num_at_signs', 'dot_number', 'num_www', 'directory_num', 'count_percent', 'count_dash', 'hostname_length', 'top_level_domain_length', 'num_letters']\n",
    "classes = df[\"type\"].unique()\n",
    "\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    \n",
    "    # First boxplot with x_train_unbalanced and y_train_unbalanced\n",
    "    data = [x_train_unbalanced[y_train_unbalanced == class_label][feature].dropna() for class_label in classes]\n",
    "    plt.boxplot(data, vert=False, patch_artist=True, showmeans=True, labels=classes)\n",
    "    plt.title(f'{feature} - Unbalanced', fontsize=10)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Second boxplot with x_train_clean_unbalanced and y_train_clean_unbalanced\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    data_clean = [x_train_clean_unbalanced[y_train_clean_unbalanced == class_label][feature].dropna() for class_label in classes]\n",
    "    plt.boxplot(data_clean, vert=False, patch_artist=True, showmeans=True, labels=classes)\n",
    "    plt.title(f'{feature} - Clean Unbalanced', fontsize=10)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one aspect that we notice from the above output is the imbalance, in terms of number of samples, of the malicious class compared to the benign one.\n",
    "from utils.utils import balance_data_undersample_benign\n",
    "\n",
    "x_train, y_train = balance_data_undersample_benign(x_train_unbalanced, y_train_unbalanced)\n",
    "x_train_clean, y_train_clean = balance_data_undersample_benign(x_train_clean_unbalanced, y_train_clean_unbalanced) # senza outliers\n",
    "printInfo(\"training balanced\", y_train)\n",
    "print()\n",
    "printInfo(\"training cleaned balanced\", y_train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbalanced, with outliers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utils.utils import evaluate_model_with_cv\n",
    "\n",
    "evaluate_model_with_cv(x_train_unbalanced, y_train_unbalanced, RandomForestClassifier(), x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbalanced, w/o outliers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utils.utils import evaluate_model_with_cv\n",
    "\n",
    "evaluate_model_with_cv(x_train_clean_unbalanced, y_train_clean_unbalanced, RandomForestClassifier(), x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced, with outliers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utils.utils import evaluate_model_with_cv\n",
    "\n",
    "evaluate_model_with_cv(x_train, y_train, RandomForestClassifier(), x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced, w/o outliers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utils.utils import evaluate_model_with_cv\n",
    "\n",
    "evaluate_model_with_cv(x_train_clean, y_train_clean, RandomForestClassifier(), x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we perform a data normalization, necessary for logistic regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "COLUMNS_TO_IGNORE = ['ip_use', 'has_http', 'has_https', 'suspiciousurl'] # binary features\n",
    "\n",
    "# defining features to normalize (all except those to ignore)\n",
    "non_binary_columns = [col for col in x_train.columns if col not in COLUMNS_TO_IGNORE]\n",
    "\n",
    "# scaling with StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# training set with outliers\n",
    "x_train_unbalanced_scaled = x_train_unbalanced.copy()\n",
    "x_train_unbalanced_scaled[non_binary_columns] = scaler.fit_transform(x_train_unbalanced[non_binary_columns])\n",
    "\n",
    "# training set w/o outliers\n",
    "x_train_clean_unbalanced_scaled = x_train_clean_unbalanced.copy()\n",
    "x_train_clean_unbalanced_scaled[non_binary_columns] = scaler.fit_transform(x_train_clean_unbalanced_scaled[non_binary_columns])\n",
    "\n",
    "# test set (NOT MODIFIED)\n",
    "x_test_scaled = x_test.copy()\n",
    "x_test_scaled[non_binary_columns] = scaler.transform(x_test[non_binary_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary regression cannot accept values ​​other than 0 and 1, so we change the classes to only two.\n",
    "from utils.utils import balance_data_undersample_benign, merge_classes_to_binary\n",
    "\n",
    "x_binary_train_scaled, y_binary_train = balance_data_undersample_benign(x_train_unbalanced_scaled, merge_classes_to_binary(y_train_unbalanced))\n",
    "x_binary_train_clean_scaled, y_binary_train_clean = balance_data_undersample_benign(x_train_clean_unbalanced_scaled, merge_classes_to_binary(y_train_clean_unbalanced))\n",
    "\n",
    "y_binary_test = merge_classes_to_binary(y_test)\n",
    "\n",
    "printInfo(\"training binary balanced\", y_binary_train)\n",
    "print()\n",
    "printInfo(\"training binary cleaned balanced\", y_binary_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the distribution of features with and without outliers (it takes about 30 seconds)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "combined_data = pd.DataFrame()\n",
    "for column in non_binary_columns:\n",
    "    combined_data[column] = x_binary_train_scaled[column]\n",
    "    combined_data[\"CLEANED -- \" + column] = x_binary_train_clean_scaled[column]\n",
    "\n",
    "columns_for_plot = []\n",
    "for column in non_binary_columns:\n",
    "    columns_for_plot.append(column)\n",
    "    columns_for_plot.append(\"CLEANED -- \" + column)\n",
    "\n",
    "# Creazione del boxplot\n",
    "plt.figure(figsize=(len(columns_for_plot) * 0.5, 6))\n",
    "sns.boxplot(data=combined_data[columns_for_plot])\n",
    "plt.title('Features after normalization', fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbalanced and normalized, with outliers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils.utils import evaluate_model_with_cv\n",
    "\n",
    "evaluate_model_with_cv(x_train_unbalanced_scaled, merge_classes_to_binary(y_train_unbalanced), LogisticRegression(), x_test_scaled, y_binary_test, ['benign', 'malignant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbalanced and normalized, without outliers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils.utils import evaluate_model_with_cv\n",
    "\n",
    "evaluate_model_with_cv(x_train_clean_unbalanced_scaled, merge_classes_to_binary(y_train_clean_unbalanced), LogisticRegression(), x_test_scaled, y_binary_test, ['benign', 'malignant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced and normalized, with outliers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils.utils import evaluate_model_with_cv\n",
    "\n",
    "evaluate_model_with_cv(x_binary_train_scaled, y_binary_train, LogisticRegression(), x_test_scaled, y_binary_test, ['benign', 'malignant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced and normalized, without outliers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "evaluate_model_with_cv(x_binary_train_clean_scaled, y_binary_train_clean, LogisticRegression(), x_test_scaled, y_binary_test, ['benign', 'malignant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
